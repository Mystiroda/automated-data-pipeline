# Automated Data Pipeline

A comprehensive data processing pipeline that automatically downloads, cleans, analyzes, and visualizes datasets with zero configuration required.

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Status](https://img.shields.io/badge/status-production--ready-brightgreen)

##  Features

- **Automatic Data Download** - From URLs or predefined datasets
- **Smart Data Cleaning** - Handles missing values, duplicates, and data quality
- **SQL Database Storage** - Automatic SQLite integration with query capabilities
- **Comprehensive Analysis** - Statistical summaries, correlations, and insights
- **Automated Visualization** - Generates plots and charts automatically
- **Detailed Reporting** - Execution logs and analysis reports
- **Zero Configuration** - Works out of the box
- **Modular Design** - Easy to extend and customize

## Quick Start

### Prerequisites
- Python 3.8 or higher
- pip (Python package manager)

### Installation & Run

```bash
# 1. Clone and setup
git clone https://github.com/Mystiroda/automated-data-pipeline.git
cd automated-Data-Pipeline
pip install -r requirements.txt

# 2. Run the pipeline (uses Titanic dataset by default)
python main.py

# 3. Verify results
python verify_database.py
```

### Try Different Datasets
```bash
# Use Iris dataset
python main.py --dataset iris

# Use your own data
python main.py --url "https://example.com/your-data.csv"
```

### Project Structure
```bash
automated-data-pipeline/
â”œâ”€â”€ main.py                   # Main pipeline file
â”œâ”€â”€ config.py                 # Configuration and paths
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ verify_database.py        # Database verification
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ QUICKSTART.md             # Quick start guide
â”œâ”€â”€ PROJECT_SUMMARY.md        # Detailed project overview
â”œâ”€â”€ PIPELINE_FLOW.txt         # Execution flow
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ processed             #data after cleaning
â”‚   â””â”€â”€ raw                   #raw data
â”œâ”€â”€ Outputs
â”‚   â”œâ”€â”€ plots                 #Generated plots
â”‚   â””â”€â”€ reports               #Generated reports
â”œâ”€â”€ Src/
â”‚   â”œâ”€â”€ __init__.py           # Initialization file
â”‚   â”œâ”€â”€ data_downloader.py    # Data downloading utilities
â”‚   â”œâ”€â”€ data_cleaner.py       # Data cleaning functions
â”‚   â”œâ”€â”€ database_handler.py   # Database operations
â”‚   â”œâ”€â”€ data_analyzer.py      # Analysis functions
â”‚   â””â”€â”€ plot_generator.py     # Visualization functions
â”œâ”€â”€tests
    â””â”€â”€ ðŸ“„ test_pipeline.py   # Test suite
```

### Supported Datasets

##### Built-in Datasets
- Titanic - Passenger survival data (--dataset titanic)
- Iris - Flower measurement data (--dataset iris)

##### Custom Datasets
- Any CSV file via URL (--url "https://...")

### Customization

Modify Cleaning Strategies

Edit `config.py`:
```python
DEFAULT_CLEANING = {
    'remove_duplicates': True,
    'strip_columns': True,
    'handle_numeric_na': 'median',  # Options: 'mean', 'median', 'zero'
    'handle_categorical_na': 'mode' # Options: 'mode', 'unknown'
}
```

### Add New Datasets
Edit `config.py`:
```python
DEFAULT_DATASETS = {
    'titanic': 'https://...titanic.csv',
    'iris': 'https://...iris.csv',
    'your_dataset': 'https://...your-data.csv'  # Add your own
}
```
### Expected Outputs
| Output Type | Location           | Description                            |
| ----------- | ------------------ |----------------------------------------|
| Raw Data    | `data/raw/`        | Original downloaded dataset            |
| Cleaned Data | `data/processed/`  | Cleaned CSV after preprocessing        |
| Visualizations | `outputs/plots/`   | Automatically generated plots          |
| Reports     | `outputs/reports/` | Statistical summaries and correlations |
| Database   | `database.sqlite`  | SQLite storage of final dataset        |
| Logs      | `pipeline.log`     | Full execution logs                    |

### Future Enhancements
- Configurable YAML-based pipelines
- Auto notebook report generation
- Machine Learning model integration
- Web dashboard (Flask + Plotly)

### License
This project is licensed under the MIT License - see the LICENSE file for details.

### Acknowledgments

Built with [Pandas](https://pandas.pydata.org/), [Matplotlib](https://matplotlib.org/), [Seaborn](https://seaborn.pydata.org/), and [SQLite](https://www.sqlite.org/). Test datasets from various public sources. Inspired by real-world data engineering pipelines

